{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb11fab-6826-4a90-bf5b-fe7b0ab464bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stopwords import get_stopwords\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comfortable-billion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fec5e94-029f-4504-8cda-286d72b78412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_extend(list_of_texts):\n",
    "    tokens_list = []\n",
    "    for item in list_of_texts:\n",
    "        tokens_list.extend(str(item).split())\n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "def cluster_printer(labels, texts, path_to_save, n_words=10):\n",
    "    label_text = defaultdict(list,{l: [] for l in labels})\n",
    "    sw = get_stopwords('russian')\n",
    "    sw += ['могу']\n",
    "    for text, label in zip(texts, labels):\n",
    "        label_text[label].append(text)\n",
    "    cluster_names = dict()\n",
    "    df = pd.DataFrame()\n",
    "    labels, sizes, word_cloud = [], [], []\n",
    "    for label, text in label_text.items():\n",
    "        corpus = tokenize_extend(text)\n",
    "        collected = 0\n",
    "        result_name = ''\n",
    "        for word, count in Counter(corpus).most_common(100):\n",
    "            if collected >= n_words:\n",
    "                break\n",
    "            if word not in sw:\n",
    "                result_name += word + '_' + str(count) + ' | '\n",
    "                collected += 1\n",
    "        labels.append(label)\n",
    "        sizes.append(len(text))\n",
    "        word_cloud.append(result_name)\n",
    "        cluster_names[f'label_{label}_size_{len(text)}'] = result_name\n",
    "    df = pd.DataFrame([labels, sizes, word_cloud]).T\n",
    "    df.columns = ['labels', 'sizes', 'words']\n",
    "    df = df.sort_values(by='sizes', ascending=False, ignore_index=True)\n",
    "    df.to_excel(os.path.join(path_to_save, f'clusterization_{len(set(labels))}_{FLAG}.xlsx'), index=False)\n",
    "    result = dict(sorted(cluster_names.items(), key=lambda x: int(x[0].split('_')[-1]), reverse=True))\n",
    "    return result\n",
    "\n",
    "\n",
    "def clusterize_viz(spec, X_lite, texts_path, n_docs_from, n_docs_to, n_words, path_to_save):\n",
    "    \n",
    "    text_df = pd.read_csv(texts_path).iloc[n_docs_from:n_docs_to]\n",
    "    texts = text_df.messages_clean.values.tolist()\n",
    "    scaler = StandardScaler()\n",
    "    tsne = TSNE(random_state=17)\n",
    "    data_scaled = scaler.fit_transform(X_lite)\n",
    "    tsne_data = tsne.fit_transform(X_lite)\n",
    "    plt.figure(figsize=(6, 6), dpi=80)\n",
    "    plt.title(\"t-SNE\")\n",
    "    plt.scatter(tsne_data[:, 0], tsne_data[:, 1], alpha=0.5,s=15)\n",
    "    labels = spec.fit_predict(X_lite)\n",
    "    text_df['labels'] = labels\n",
    "    new_path =  texts_path.split('.')[0].split('/')[-1] + f'_annotated_{FLAG}.xlsx'\n",
    "    texts_path_modified = os.path.join(path_to_save, new_path)\n",
    "    text_df.to_excel(texts_path_modified, index=False)\n",
    "    print(\"silhouette_score = \", silhouette_score(X_lite, labels, random_state=666))\n",
    "    \n",
    "    ngram_pd=pd.DataFrame(X_lite)\n",
    "    vec_numpy = tsne.fit_transform(ngram_pd)\n",
    "    vec_numpy = pd.DataFrame(vec_numpy)\n",
    "    vec_numpy['label'] = spec.labels_\n",
    "    cluster_mean=vec_numpy.groupby(['label']).mean().reset_index()\n",
    "    count_array= np.empty(np.unique(labels).shape[0])\n",
    "    print(\"number clusters = \", np.unique(labels).shape[0])\n",
    "    \n",
    "    for label in range(np.unique(labels).shape[0]):   #количество векторов в каждом кластере\n",
    "        occurences=np.count_nonzero(spec.labels_ == label)\n",
    "        count_array[label]=occurences\n",
    "    cluster_mean['count']=count_array  \n",
    "    cluster_mean.columns=['label','x','y','count']\n",
    "    \n",
    "    fig = px.scatter(data_frame=cluster_mean, \n",
    "                     x=cluster_mean['x'], \n",
    "                     y=cluster_mean['y'], \n",
    "                     size=\"count\",\n",
    "                     hover_name=\"label\")\n",
    "\n",
    "    cluster_meta = cluster_printer(labels, texts, path_to_save, n_words)\n",
    "    fig.show()\n",
    "    return cluster_meta\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "swiss-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs_from = 0\n",
    "n_docs_to = 10_000\n",
    "n_words = 10\n",
    "distance_threshold = 0.4\n",
    "\n",
    "\n",
    "embeddings_path = '/hdd/docker_shared_folder/chatbot-data/embeddings/rubert/ru_emb_100000_historical-chatbot-diarized.csv'\n",
    "texts_path      = '/hdd/docker_shared_folder/chatbot-data/data/historical-chatbot-diarized.csv'\n",
    "path_to_save    = '/hdd/docker_shared_folder/chatbot-data/clusterization'\n",
    "FLAG            = f'almat_cb_{n_docs_from}_{n_docs_to}'\n",
    "\n",
    "\n",
    "X = np.fromfile(embeddings_path, dtype=float, sep=\"\\n\")\n",
    "\n",
    "dim = 768\n",
    "X.resize(X.shape[0] // dim, dim)\n",
    "X = X[n_docs_from:n_docs_to]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188acd8-7379-457a-95c6-60102595a8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silhouette_score =  0.03300577778238554\n"
     ]
    }
   ],
   "source": [
    "agl = AgglomerativeClustering(\n",
    "                affinity=\"cosine\", \n",
    "                distance_threshold=0.3, \n",
    "                n_clusters=None, \n",
    "                linkage=\"complete\"\n",
    ")\n",
    "\n",
    "\n",
    "cluster_meta = clusterize_viz(spec=agl, \n",
    "                              X_lite=X, \n",
    "                              texts_path=texts_path, \n",
    "                              n_docs_from=n_docs_from, \n",
    "                              n_docs_to=n_docs_to, \n",
    "                              n_words=n_words,\n",
    "                              path_to_save=path_to_save\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9d60c-4b20-45ba-91a6-59a991df4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in cluster_meta.items():\n",
    "    print(key, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnd_ds",
   "language": "python",
   "name": "rnd_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
